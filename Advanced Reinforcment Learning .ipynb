{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lsrzMdh_lEym"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q_ai8FClFlx",
        "outputId": "43d31f4b-67f6-4b30-8c86-53e5b157358f"
      },
      "outputs": [],
      "source": [
        "img_folder = 'random_prompts/'\n",
        "\n",
        "prompts = [\"A woman holding a bouquet of flowers and a puppy in a park\",\n",
        "           \"A group of friends having a picnic in the countryside\",\n",
        "           \"A couple dancing at a wedding reception\",\n",
        "           \"A family walking on a beach during sunset\",\n",
        "           \"A person practicing yoga in a peaceful garden\",\n",
        "           \"A child blowing out birthday candles on a cake\",\n",
        "           \"A group of people enjoying a concert in a stadium\",\n",
        "           \"A man proposing to his girlfriend on a mountain top\",\n",
        "           \"A mother and daughter baking cookies in the kitchen\", \n",
        "           \"A businessman shaking hands with a client in an office\",\n",
        "           #\"A woman trying on dresses at a clothing store\",\n",
        "           \"A family having a barbecue in their backyard\",\n",
        "           \"A group of coworkers celebrating a successful project\",\n",
        "           #\"A couple enjoying a romantic dinner at a fancy restaurant\", \n",
        "           \"A person taking a selfie with a famous landmark\", \n",
        "           \"A mother and son playing in a park\", \n",
        "           \"A woman getting a haircut at a salon\",\n",
        "           \"A group of friends playing volleyball on the beach\",\n",
        "           \"A man riding a bike through a city park\",\n",
        "           \"A woman enjoying a book and a cup of tea in a cozy living room\",\n",
        "          # \"A cat sitting on a windowsill, looking out at a city skyline at night\",    \n",
        "           \"A woman walking her dog on a beach at sunset\",    \n",
        "           \"A group of friends hiking in a forest with a waterfall in the background\", \n",
        "           \"A person skateboarding in a city park with skyscrapers in the background\",  \n",
        "           \"A group of people kayaking down a river surrounded by mountains\",   \n",
        "           \"A couple walking hand in hand through a winter wonderland\",    \n",
        "           \"A person reading a book on a hammock in a tropical garden\",   \n",
        "           \"A group of friends having a barbecue on a rooftop terrace\",   \n",
        "           #\"A person scuba diving in a coral reef surrounded by tropical fish\",  \n",
        "           \"A couple riding bicycles on a scenic country road\",   \n",
        "           \"A person practicing yoga on a beach at sunrise\",  \n",
        "           \"A group of people enjoying a boat ride on a calm lake\",\n",
        "          \"A nurse administering medication to a patient\",\n",
        "          \"A chef preparing a meal in a restaurant kitchen\",\n",
        "          \"A parent reading a bedtime story to their child\",\n",
        "          \"A scientist observing a chemical reaction in a lab\",\n",
        "          \"A therapist counseling a client in their office\",\n",
        "          \"A teacher instructing a class of students\",\n",
        "          \"A coach training an athlete at the gym\",\n",
        "          \"A firefighter rescuing a person from a burning building\",\n",
        "          \"A musician performing on stage for a live audience\",\n",
        "          \"A police officer patrolling a neighborhood\",\n",
        "          \"A surgeon performing a delicate operation\",\n",
        "          \"A construction worker laying bricks on a building site\",\n",
        "          #\"A farmer harvesting crops in a field\",\n",
        "          \"A flight attendant assisting passengers on a plane\",\n",
        "          \"A veterinarian examining a sick animal\",\n",
        "          \"A soldier serving in a war zone\",\n",
        "          \"A salesperson closing a deal with a customer\",\n",
        "          \"A journalist conducting an interview with a celebrity\",\n",
        "          \"A therapist leading a group therapy session\",\n",
        "        #  \"A serene forest with a babbling brook\",\n",
        "        #  \"An otherworldly planet with two suns in the sky\",    \n",
        "         # \"A cozy cottage nestled in the mountains\",    \n",
        "        #  \"A futuristic city with towering skyscrapers\",   \n",
        "        #  \"An abandoned amusement park overrun by nature\",   \n",
        "        #  \"A vibrant coral reef teeming with marine life\",    \n",
        "        #  \"A magical garden with talking flowers and plants\",  \n",
        "        #  \"A mysterious cave system with glowing crystals\",   \n",
        "         # \"A desert oasis with a shimmering pool of water\",    \n",
        "        #  \"A snowy mountain peak with a winding trail\",    \n",
        "        #  \"A peaceful countryside scene with grazing animals\", \n",
        "        #  \"A bustling marketplace with exotic goods and foods\",   \n",
        "        #  \"A futuristic spacecraft exploring deep space\", \n",
        "        #  \"A tropical island with a hidden waterfall\",  \n",
        "         # \"A haunted mansion with creaky doors and ghostly figures\",  \n",
        "         # \"A prehistoric landscape with towering dinosaurs\", \n",
        "         # \"A massive ocean with a school of whales breaching\", \n",
        "         # \"A dystopian cityscape with towering walls and surveillance drones\",  \n",
        "         # \"A lush rainforest with a canopy of trees and colorful birds\", \n",
        "         # \"A fantastical landscape with floating islands and castles\",\n",
        "         # \"A vintage typewriter on a wooden desk\",\n",
        "         # \"A sleek sports car speeding down a city street\",\n",
        "         # \"A classic record player with vinyl records\",\n",
        "         # \"A colorful bouquet of flowers in a vase\",\n",
        "         # \"A collection of antique books on a shelf\",\n",
        "          #\"A retro telephone with a rotary dial\",\n",
        "         # \"A modern smartphone with a vibrant screen\",\n",
        "         # \"A steaming cup of coffee on a saucer\",\n",
        "         # \"A stylish wristwatch with a leather strap\",\n",
        "         # \"A set of professional makeup brushes\",\n",
        "          #\"A sparkling diamond engagement ring\",\n",
        "        #  \"A rustic wooden rocking chair on a porch\",\n",
        "        #  \"A classic acoustic guitar with a pick\",\n",
        "        #  \"A vintage camera with interchangeable lenses\",\n",
        "        #  \"A set of antique silverware on a table\",\n",
        "         # \"A luxurious leather handbag with gold hardware\",\n",
        "         # \"A cozy knit sweater in a neutral color\",\n",
        "         # \"A pair of sleek black high-heeled shoes\",\n",
        "        #  \"A trendy pair of wireless headphones\",\n",
        "        #  \"A set of colorful markers and sketchpads\",  \n",
        "         #  \"A dragon breathing fire and ice at the same time\",    \n",
        "         #  \"A giant robotic octopus attacking a city\",   \n",
        "         #  \"A treehouse floating in mid-air with balloons\",  \n",
        "          # \"A time-traveling hot air balloon made of glass\",   \n",
        "         #  \"A mermaid riding a unicorn through the clouds\",  \n",
        "         #  \"A robot DJ spinning tracks on a turntable\",    \n",
        "         #  \"A giant snail racing against a rocket ship\",   \n",
        "        #   \"A squad of superhero kittens fighting crime\",   \n",
        "         #  \"A wizard battling a giant mechanical spider\",  \n",
        "        #   \"A giant slug wearing a top hat and monocle\",   \n",
        "        #   \"A roller coaster track made of jelly\",   \n",
        "        #   \"A submarine made entirely of candy\",   \n",
        "         #  \"A massive game of Jenga played with skyscrapers\", \n",
        "         #  \"A team of racing sloths competing in a Grand Prix\", \n",
        "         #  \"A dragonfly with the wings of a butterfly\",    \n",
        "         #  \"A carnivorous plant with a friendly personality\", \n",
        "         #  \"A robot elephant painting a mural on a building\",  \n",
        "        #   \"A giant rainbow-colored mushroom in a forest\",  \n",
        "         #  \"A pirate ship sailing through a city street\",\n",
        "         #  \"A misty mountain peak at sunrise\",  \n",
        "         #  \"A field of lavender in the golden hour\",   \n",
        "         #  \"A pastel-colored city skyline at night\",   \n",
        "         #  \"A glittering cityscape reflected on a river\",  \n",
        "         #  \"A minimalist interior with natural lighting\",   \n",
        "          # \"A symmetrical garden with perfectly trimmed hedges\",  \n",
        "         #  \"A vibrant coral reef with tropical fish\",   \n",
        "        #   \"A tranquil forest with a shimmering pond\",  \n",
        "         #  \"A sunset over a calm ocean horizon\",    \n",
        "         #  \"A cascading waterfall in a lush rainforest\",  \n",
        "         #  \"A bright rainbow in a clear blue sky\", \n",
        "         #  \"A sparkling city street after a rain shower\", \n",
        "         #  \"A whimsical hot air balloon festival\",  \n",
        "         #  \"A cozy cafe with warm lighting and decor\", \n",
        "         #  \"A stunning coastal cliffside at sunset\", \n",
        "         #  \"A tranquil zen garden with raked sand\",  \n",
        "         #  \"A whimsical carousel with colorful lights\",  \n",
        "         #  \"A futuristic cityscape with neon lights\",  \n",
        "         #  \"A luxurious yacht sailing through crystal clear waters\", \n",
        "         #  \"A snowy forest with a serene atmosphere\",\n",
        "            \"A bride and groom sharing a first dance at their wedding\",    \n",
        "           \"A group of athletes competing in a high-intensity sports game\",  \n",
        "           \"A family gathered around a cozy fireplace on a winter evening\",  \n",
        "           \"A surfer catching a wave at a tropical beach\",  \n",
        "           \"A group of friends enjoying a night out on the town\", \n",
        "           \"A fashion model posing for a photo shoot on a city street\",\n",
        "           \"A mother and child playing in a park on a sunny day\",  \n",
        "           \"A traveler exploring a new city and experiencing its culture\",  \n",
        "           \"A firefighter battling a raging inferno in a burning building\", \n",
        "           \"A teacher inspiring and educating a classroom of students\",  \n",
        "           \"A couple strolling hand-in-hand through a blooming garden\",  \n",
        "           \"A group of musicians jamming together in a garage band\",   \n",
        "           #\"A diver swimming with colorful tropical fish in a coral reef\",    \n",
        "           \"A dancer performing a passionate and expressive routine\",   \n",
        "           \"A scientist researching and discovering new breakthroughs\",   \n",
        "           \"A parent reading a bedtime story to their child\",  \n",
        "           \"A cyclist racing in a high-speed bike competition\",  \n",
        "           \"A group of friends laughing and having fun at a party\", \n",
        "           \"A chef presenting a beautiful and delicious dish at a restaurant\",  \n",
        "           \"A soldier bravely serving their country in a warzone\",  \n",
        "           \"A couple enjoying a sunset on a secluded beach\",    \n",
        "           \"A businessperson closing a major deal in a boardroom\", \n",
        "           \"A group of volunteers helping and serving their community\", \n",
        "           \"A doctor comforting and treating a sick patient in a hospital\", \n",
        "           \"A fashion designer sketching out their latest designs\", \n",
        "           \"A dancer practicing in a dimly lit studio\",  \n",
        "           \"A group of tourists taking in the sights and sounds of a new city\",   \n",
        "           \"A student studying and preparing for a big exam\",  \n",
        "           \"A cyclist enjoying a leisurely ride through a scenic countryside\",  \n",
        "           \"A group of friends enjoying a game night at home\",  \n",
        "           \"A scientist examining a microscope slide in a lab\",   \n",
        "           \"A couple enjoying a hot air balloon ride over a picturesque landscape\",  \n",
        "           \"A businessperson giving a powerful and inspiring presentation\",   \n",
        "           \"A mother comforting her child after a bad dream\",  \n",
        "           \"A surfer catching a sunset wave in the ocean\",   \n",
        "           \"A firefighter rescuing a person from a burning building\",    \n",
        "           \"A teacher guiding and mentoring a student one-on-one\",   \n",
        "           #\"A musician performing a soulful and heartfelt ballad\",    \n",
        "           \"A group of athletes practicing and training together\"]\n",
        "\n",
        "\n",
        "prompts_length = len(prompts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KnRsSlZOHI8x"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rgckSZ1YDz_5",
        "outputId": "9c7d5626-6cf2-4771-a0a3-e3522c7469b6"
      },
      "outputs": [],
      "source": [
        "#!pip install diffusers\n",
        "#!pip install transformers scipy ftfy accelerate\n",
        "#!pip install git+https://github.com/openai/CLIP.git --run at least once!\n",
        "\n",
        "#Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import platform\n",
        "#import torchvision.transforms as transforms\n",
        "import clip\n",
        "import cv2\n",
        "from PIL import ImageChops, Image\n",
        "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
        "import random\n",
        "#import torch\n",
        "import sys\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import re\n",
        "import time\n",
        "from os.path import exists\n",
        "from os import mkdir\n",
        "import argparse\n",
        "\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Canny Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_image(image_path): #encode image to 512 vector\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model, preprocess = clip.load('ViT-B/32', device=device)\n",
        "    \n",
        "    # Load the image and preprocess it\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((224, 224))\n",
        "    image = preprocess(image).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "    image_features = image_features[0] # flatten the feature tensor\n",
        "    return image_features \n",
        "\n",
        "def encode_images(image_paths):\n",
        "    #encode images to 512 vectors\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model, preprocess = clip.load('ViT-B/32', device=device)\n",
        "    \n",
        "    # Load the images and preprocess them\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize((224, 224))\n",
        "        image = preprocess(image)\n",
        "        images.append(image)\n",
        "    images = torch.stack(images)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(images)\n",
        "    image_features = image_features.view(image_features.shape[0], -1) # flatten the feature tensor\n",
        "    return image_features\n",
        "\n",
        "def MLP(input_dim, output_dim, hidden_layers_dims, activation, bias):\n",
        "    \"\"\"This function creates a multi-layer perceptron.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): The input dimension of the MLP.\n",
        "        output_dim (int): The output dimension of the MLP.\n",
        "        hidden_layers_dims (list): A list of integers that represent the number of hidden layers and their dimensions.\n",
        "        activation (nn.functional, optional): The activation function to use. Defaults to tanh.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The MLP\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(input_dim, hidden_layers_dims[0], bias=bias))\n",
        "    layers.append(activation)\n",
        "    for i in range(1, len(hidden_layers_dims)):\n",
        "        layers.append(nn.Linear(hidden_layers_dims[i - 1], hidden_layers_dims[i], bias=bias))\n",
        "        layers.append(activation)\n",
        "    layers.append(nn.Linear(hidden_layers_dims[-1], output_dim, bias=bias))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class CannyModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim=512, output_dim=4, hidden_layers_dims=[16,16], activation=nn.ReLU()):\n",
        "        #TODO: ReLu activation fct?\n",
        "        #NOTE: input_dims shall correspond to dimension output of the textEncoder (here 512 fits the output of the clip Text encoder used below)\n",
        "        # output_dim shall corresponds to the number of parameters at the output we want: here 4 because 4 canny parameters\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        super().__init__()\n",
        "\n",
        "        # Define a neural MLP network by specifying number of inputs neurons output neurons \n",
        "        # and number neurons in intermediate layers (hidden layer)\n",
        "        self.model=MLP(input_dim, output_dim, hidden_layers_dims, activation, bias=True)\n",
        "        self.model.to(device)\n",
        "\n",
        "    def forward_1img(self, image):       \n",
        "        image_feature_vector=encode_image(image) #image to feature vector by passing through pretrained model\n",
        "        canny_parameters=self.model(image_feature_vector)#image feature vector to canny parameters by passing through model\n",
        "        return canny_parameters \n",
        "        \n",
        "    \n",
        "    def forward(self, images):       \n",
        "        image_feature_vectors = encode_images(images) #images to feature vectors by passing through pretrained model\n",
        "        canny_parameters = self.model(image_feature_vectors) #feature vectors to canny parameters by passing through model\n",
        "        return canny_parameters\n",
        "    \n",
        "   \n",
        "\n",
        "    def get_weights(self):\n",
        "        param = nn.utils.parameters_to_vector(\n",
        "            self.parameters()).detach().numpy()\n",
        "        return param"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7oFcJVbLlz5Q"
      },
      "source": [
        "# Segmentation + Canny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFeYtpyPl4C8"
      },
      "outputs": [],
      "source": [
        "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
        "\n",
        "def segment(image):\n",
        "\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
        "\n",
        "    # Define the masks\n",
        "    person_mask = (predictions == 12).astype(np.uint8)\n",
        "    other_mask = (predictions != 12).astype(np.uint8)\n",
        "\n",
        "    # Resize the masks to match the original image size\n",
        "    person_mask = np.array(Image.fromarray(person_mask).resize(image.size, resample=Image.NEAREST))\n",
        "    other_mask = np.array(Image.fromarray(other_mask).resize(image.size, resample=Image.NEAREST))\n",
        "\n",
        "    # Apply the masks to the original image and save the results\n",
        "    person_image = np.array(image) * person_mask[..., np.newaxis]\n",
        "    other_image = np.array(image) * other_mask[..., np.newaxis]\n",
        "\n",
        "    person_image_rgb = person_image[...,::-1] * person_mask[..., np.newaxis]\n",
        "    other_image_rgb = other_image[...,::-1] * other_mask[..., np.newaxis]\n",
        "\n",
        "    person_image_rgba = np.concatenate([person_image_rgb, person_mask[..., np.newaxis] * 255], axis=-1)\n",
        "    other_image_rgba = np.concatenate([other_image_rgb, other_mask[..., np.newaxis] * 255], axis=-1)\n",
        "\n",
        "    return Image.fromarray(other_image_rgba.astype(np.uint8)),Image.fromarray(person_image_rgba.astype(np.uint8))\n",
        "\n",
        "def canny_edge(low_human,high_human,low_other,high_other,other_image,human_image=None):\n",
        "    other_image.save(\"other.png\")\n",
        "    \n",
        "    #Edge detection of others\n",
        "    img = cv2.imread(\"other.png\") \n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "    img = cv2.Canny(img, low_other,high_other)\n",
        "    img = 255 - img \n",
        "    cv2.imwrite(\"image_other_edge.png\",img)\n",
        "\n",
        "    if (human_image != None): \n",
        "        #Edge detection of human\n",
        "        human_image.save(\"human.png\")\n",
        "        img = cv2.imread(\"human.png\")  \n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        img = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "        img = cv2.Canny(img, low_human,high_human)\n",
        "        img = 255 - img\n",
        "        cv2.imwrite(\"image_human_edge.png\",img)\n",
        "        \n",
        "        #Blending of images\n",
        "        blended = Image.open(\"image_other_edge.png\")\n",
        "        human_img = Image.open(\"image_human_edge.png\")\n",
        "        blended = ImageChops.darker(blended,human_img)\n",
        "    else: \n",
        "        blended = Image.open(\"image_other_edge.png\")\n",
        "    return blended\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R6UrP_TxHajd"
      },
      "source": [
        "# Fitness function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "detq99QgDiIT"
      },
      "outputs": [],
      "source": [
        "def split_long(s): #splits long prompts so that they are shown nicely by the image\n",
        "    pattern = r'.{1,20}(?=\\s|$)'\n",
        "    s = re.sub(pattern, r'\\g<0>\\n', s)\n",
        "    return s\n",
        "  \n",
        "def shift_params(canny_params): #shifts the parameters so that they are in the range closer to the parameters we want\n",
        "    shifted_x = canny_params - torch.min(canny_params.abs())\n",
        "    shifted_x = torch.abs(shifted_x)\n",
        "    shifted_x[1] = shifted_x[0]+shifted_x[1] #making sure high is higher than low\n",
        "    shifted_x[3] = shifted_x[2]+shifted_x[3] #making sure high is higher than low\n",
        "    return shifted_x*12\n",
        "    \n",
        "def get_prompts(): #get three random prompts \n",
        "    rand = random.sample(range(1, prompts_length-1), 5)\n",
        "    for i in range(5):\n",
        "        rand[i] = prompts[rand[i]]\n",
        "    return rand\n",
        "\n",
        "def add_img_end(list):\n",
        "    list_ = []\n",
        "    for i in list:\n",
        "        list_.append(img_folder+i+\"_50.png\")\n",
        "    return list_\n",
        "\n",
        "def get_image(prompt): #get image from prompt\n",
        "    return Image.open(img_folder+prompt+\"_50.png\")   \n",
        "\n",
        "def play_sound():\n",
        "    os_name = platform.system()\n",
        "    if (os_name==\"Windows\"):\n",
        "        import winsound\n",
        "        duration = 1000  # milliseconds\n",
        "        freq = 440  # Hz\n",
        "        winsound.Beep(freq, duration)\n",
        "    elif (os_name==\"Darwin\"):\n",
        "        import simpleaudio as sa\n",
        "        freq = 440  # Our played note will be 440 Hz\n",
        "        fs = 44100  # 44100 samples per second\n",
        "        seconds = 1  # Note duration of 1 second\n",
        "\n",
        "        # Start playback\n",
        "        # Generate array with seconds*sample_rate steps, ranging between 0 and seconds\n",
        "        t = np.linspace(0, seconds, seconds * fs, False)\n",
        "\n",
        "        # Generate a 440 Hz sine wave\n",
        "        note = np.sin(freq * t * 2 * np.pi)\n",
        "\n",
        "        # Ensure that highest value is in 16-bit range\n",
        "        audio = note * (2**15 - 1) / np.max(np.abs(note))\n",
        "        # Convert to 16-bit data\n",
        "        audio = audio.astype(np.int16)\n",
        "        play_obj = sa.play_buffer(audio, 1, 2, fs)\n",
        "\n",
        "        # Wait for playback to finish before exiting\n",
        "        play_obj.wait_done()\n",
        "    \n",
        "\n",
        "def fitness_batch(evolved_parameters: np.array, args=None) -> float:\n",
        "    \"\"\"\n",
        "    For an interactive evolution, with N(=num_candidates) structures compared.\n",
        "    Ask the human to select a structure among N structures generated each by a different policy network (MLP).\n",
        "    input:\n",
        "        evolved_parameters: np.array of size num_candidates * N_policy_param(>>), gathering the parameters of the num_candidates policy network compared here.\n",
        "        args: dictionary of parameters given as input of train\n",
        "        model: Image model..\n",
        "    output:\n",
        "        rewards: list of size num_candidates of rewards for each of these networks.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    #number of time will ask human rate the structures per generation: depends population size and num_candidates\n",
        "    num_rounds=math.floor(args.population_size/args.num_candidates)\n",
        "    rewards = [0]*args.population_size\n",
        "    num_prompts=5\n",
        "    images_show=[] #contain images all candidates                         \n",
        "\n",
        "    # Loop on the whole structure\n",
        "    print(\"About to Evaluate {} elements in {} rounds with {} prompts\".format(args.population_size, num_rounds, num_prompts))\n",
        "    for j in range(0,num_rounds):\n",
        "        prompts_for_round = get_prompts()\n",
        "        images=[] #contain images all candidates                         \n",
        "        img_to_canny = [] #contain original images to be canny edge detected\n",
        "        other_seg = [] #list of segmented parts of other\n",
        "        human_seg = [] #list of segmented part of humans\n",
        "\n",
        "         #---1---Create Image for all candidates and all prompts \n",
        "        for k in range(0,args.num_candidates):   \n",
        "            \n",
        "            #find pram_candidate in evolved_parameters for each candidate\n",
        "            idx_candidate_in_pop= j*args.num_candidates+k\n",
        "            nn_candidate=evolved_parameters[idx_candidate_in_pop]   \n",
        "            parameters_candidate = nn_candidate(add_img_end(prompts_for_round))\n",
        "            canny_params = parameters_candidate[i]\n",
        "            canny_params = shift_params(canny_params)\n",
        "            images_=[]\n",
        "            for i in range(0,num_prompts):  \n",
        "                if(k==0):\n",
        "                    image = get_image(prompts_for_round[i])#gør det bedre her?                \n",
        "                    img_to_canny.append(image)\n",
        "                    other_img,human_img = segment(image)\n",
        "                    other_seg.append(other_img)\n",
        "                    human_seg.append(human_img)                  \n",
        "                    image=canny_edge(canny_params[0].item(),canny_params[1].item(),canny_params[2].item(),canny_params[3].item(),other_img,human_img) #canny edge detect image\n",
        "                    \n",
        "                else: \n",
        "                    image = canny_edge(canny_params[0].item(),canny_params[1].item(),canny_params[2].item(),canny_params[3].item(),other_seg[i],human_seg[i]) #canny edge detect image\n",
        "                \n",
        "                #so PIL errors don't occur\n",
        "                plt.imshow(image)\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "                if(args.clear_output):\n",
        "                    clear_output(wait=True)\n",
        "\n",
        "                images_.append(image)\n",
        "            images.append(images_)\n",
        "        images_show.append(images)\n",
        "          \n",
        "    for j in range(0,num_rounds):\n",
        "        print(images_show[j])\n",
        "        #---2--- Display these images and ask human to rank these candidates\n",
        "        fig,axes = plt.subplots(nrows=num_prompts, ncols=args.num_candidates, figsize=(10, 10))\n",
        "       # plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.1, hspace=0.1)\n",
        "        for ii in range(0, args.num_candidates):\n",
        "            for jj in range(0, num_prompts):\n",
        "                axes[jj,ii].imshow(images_show[j][ii][jj],cmap=\"gray\")#ith candidate and jth prompt\n",
        "        plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "        columnLabels = ['Candidate {}'.format(col) for col in range(1, args.num_candidates+1)]\n",
        "        rowLabels = prompts_for_round \n",
        "        for ax, label in zip(axes[0], columnLabels):\n",
        "            ax.set_title(label,fontsize=10)\n",
        "        for ax, label in zip(axes[:,0], rowLabels):\n",
        "            ax.set_ylabel(label,rotation=0,fontsize=6)\n",
        "            ax.yaxis.set_label_coords(-0.3, 0.5)\n",
        "        \n",
        "        fig.tight_layout()\n",
        "        plt.show() \n",
        "        play_sound()\n",
        "            \n",
        "        # Ask Rating Human\n",
        "        print(f\"Round {j} Choose the best of these images, from 1 to {args.num_candidates}\\n\")\n",
        "\n",
        "        while True: #in case entry not valid\n",
        "            try:\n",
        "                index = int(input()) #this is the ranking of the human\n",
        "                if index not in np.arange(args.num_candidates)+1:\n",
        "                    print('Entry not valid, try again. Press 0 to EXIT. Entry should be in {}'.format(np.arange(args.num_candidates))+1)\n",
        "                else:\n",
        "                    break\n",
        "            except:\n",
        "                print('Entry not valid, try again. Press 0 to EXIT. Entry should be in {}'.format(np.arange(args.num_candidates)+1))\n",
        "            if index == 0:\n",
        "                print(\"Finishing Evolution early\")\n",
        "                sys.exit(\"Bye\")\n",
        "        print(f\"Round {j} Human has selected entry {index}\\n\")\n",
        "        #---3--- Update Reward\n",
        "        plt.close()\n",
        "        # Gives high reward to selected candidate\n",
        "        idx_selected_candidate_in_pop = j*args.num_candidates+index-1\n",
        "        print(\"Good Candidate element {} / {}\".format(idx_selected_candidate_in_pop+1, args.population_size))\n",
        "        rewards[idx_selected_candidate_in_pop] = 1\n",
        "        if(args.clear_output):\n",
        "            clear_output(wait=True)\n",
        "    return rewards  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K3wSZm3WHO82"
      },
      "source": [
        "# Evolutionary strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_k9EKpODaBE"
      },
      "outputs": [],
      "source": [
        "class EvolutionStrategyStatic(object):\n",
        "    def __init__(self, args): \n",
        "        self.args = args\n",
        "        self.get_reward = fitness_batch\n",
        "        if (args.new_run == None):\n",
        "            self.generation = 0\n",
        "            self.population_size = args.population_size\n",
        "            self.sigma = args.sigma\n",
        "            self.lr = args.lr\n",
        "            self.lr_decay = args.lr_decay\n",
        "            #self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA) ?? ikke nødvenidg her? \n",
        "            self.batch_size=args.num_candidates\n",
        "            self.used_population = []\n",
        "            self.sigma_decay = args.sigma_decay\n",
        "            self.weights = self.init_weights()\n",
        "            self.id_ = str(int(time.time()))\n",
        "        else:\n",
        "            dict = torch.load(\"weights/\"+args.new_run)\n",
        "            self.generation = dict[\"generation\"]+1\n",
        "            self.sigma = dict[\"sigma\"]\n",
        "            self.population_size = dict[\"population_size\"]\n",
        "            self.lr = dict[\"lr\"]\n",
        "            self.lr_decay = dict[\"lr_decay\"]\n",
        "            self.batch_size = dict[\"batch_size\"]\n",
        "            self.used_population = dict[\"used_population\"]\n",
        "            self.sigma_decay = dict[\"sigma_decay\"]\n",
        "            self.weights = dict[\"weights\"]\n",
        "            self.id_ = dict[\"id_\"]\n",
        "        \n",
        "    def to_dict(self):\n",
        "        return {key: value for key, value in self.__dict__.items() if not key.startswith('__') and not callable(key)}\n",
        "\n",
        "    def init_weights(self): #get initial weights, which is set randomly\n",
        "        \"\"\" canny=CannyModel()\n",
        "        weight_dim=512*16+16*16+16*4\n",
        "        bias_dim=16+16+4\n",
        "        evolved_parameters=(2*np.random.rand((weight_dim+bias_dim))-1)\n",
        "        nn.utils.vector_to_parameters(torch.tensor(evolved_parameters, dtype=torch.float32),  canny.parameters()) \"\"\"\n",
        "        canny = torch.load(\"starting_point_RL2_bad.pt\")\n",
        "        return canny \n",
        "    \n",
        "    def get_NN(self, w, p): # get nerual network based on weights and noise vector \n",
        "        #p is np array noise vector for one element in the population\n",
        "        #w is the mean weight vector\n",
        "        canny=CannyModel()\n",
        "        evolved_parameters=w.get_weights()+self.sigma*p\n",
        "        nn.utils.vector_to_parameters(torch.tensor(evolved_parameters, dtype=torch.float32),  canny.parameters())\n",
        "        return canny \n",
        "\n",
        "    def get_noise_vector(self): #make a random vector of size of neural network weights\n",
        "        population = []\n",
        "        for _ in range(self.population_size):\n",
        "            weight_dim=512*16+16*16+16*4\n",
        "            bias_dim=16+16+4\n",
        "            evolved_parameters=(2*np.random.rand((weight_dim+bias_dim))-1) \n",
        "            population.append(evolved_parameters)  \n",
        "        return population  \n",
        "    \n",
        "    def evaluate(self, noise_vector): #create population and give rewards\n",
        "        rewards = []\n",
        "        population = [self.get_NN(self.weights, p) for p in noise_vector] #generate population\n",
        "        rewards=self.get_reward(population, self.args) #run fitness function to get rewards\n",
        "        rewards = np.array(rewards)\n",
        "        self.used_population = population\n",
        "        return rewards\n",
        "\n",
        "    def update_weights(self, rewards, noise_vector): #update weights based on rewards \n",
        "        weight_parameters = self.weights.get_weights() \n",
        "        \n",
        "        for index, w in enumerate(weight_parameters): \n",
        "            layer_noise_vector = np.array([p[index] for p in noise_vector])   # Array of all weights[i] for all the networks in the population\n",
        "            self.update_factor = self.lr / (self.population_size * self.sigma)\n",
        "            weight_parameters[index] = w + self.update_factor * np.dot(layer_noise_vector.T, rewards).T #for each element in the weight vector, update it based on the rewards of the population\n",
        "       \n",
        "        nn.utils.vector_to_parameters(torch.tensor(weight_parameters, dtype=torch.float32),  self.weights.parameters()) #update weights parameters\n",
        "        \n",
        "        if self.update_factor > 0.001: #decay learning rate\n",
        "            self.lr *= self.lr_decay\n",
        "\n",
        "        if self.sigma>0.001: #decay sigma\n",
        "            self.sigma *= self.sigma_decay\n",
        "\n",
        "\n",
        "    def run(self, generations, path='outputs'):\n",
        "        if not exists(path + '/' + self.id_):\n",
        "            mkdir(path + '/' + self.id_)\n",
        "\n",
        "        if self.generation == 0: \n",
        "            with open(path +\"/\" + self.id_ + \"/\"+ \"args.txt\", 'w') as output:\n",
        "                args_dict = vars(self.args)\n",
        "                for key in args_dict:\n",
        "                    output.write(key + \" : \" + str(args_dict[key])+\"\\n\")\n",
        "            \n",
        "            torch.save(self.weights, path +\"/\" + self.id_ + \"/\"+\"weights_gen_{:02d}__\".format(self.generation)+   \"sigma_\" + str(self.sigma)[:4] + \"__lr_\" + str(self.lr)[:6] + \".pt\")\n",
        "            torch.save(self.used_population, path + \"/\"+ self.id_ + \"/\" +\"used_population_gen_{:02d}__\".format(self.generation)+ \"sigma_\" + str(self.sigma)[:4] + \"__lr_\" + str(self.lr)[:6] + \".pt\")  \n",
        "            torch.save(self.to_dict(),path +\"/\" + self.id_ + \"/\"+\"state_gen_{:02d}.pt\".format(self.generation))\n",
        "\n",
        "            \n",
        "        print('\\n********************\\n \\n Evolutionary RUN: ' + self.id_ + '\\n\\n********************\\n')\n",
        "        # Algorithm 2. Salimans, 2017: https://arxiv.org/abs/1703.03864\n",
        "        \n",
        "         #current index of the generation\n",
        "\n",
        "        while self.generation < generations:                                    \n",
        "            #1--- get current noise_vector\n",
        "            noise_vector = self.get_noise_vector()                            \n",
        "            #2-- evaluate reward\n",
        "            rewards = self.evaluate(noise_vector)\n",
        "                     \n",
        "            #3--mutate parameters for next generation\n",
        "            self.update_weights(rewards, noise_vector)\n",
        "            \n",
        "            self.generation += 1\n",
        "            #save weights, population, and args\n",
        "            print('iter %4i | update_factor: %f  lr: %f  sigma: %f |' % (self.generation + 1, self.update_factor, self.lr, self.sigma), flush=True)\n",
        "            torch.save(self.weights, path +\"/\" + self.id_ + \"/\"+\"weights_gen_{:02d}__\".format(self.generation)+   \"sigma_\" + str(self.sigma)[:4] + \"__lr_\" + str(self.lr)[:6] + \".pt\")\n",
        "            torch.save(self.used_population, path + \"/\"+ self.id_ + \"/\" +\"used_population_gen_{:02d}__\".format(self.generation)+ \"sigma_\" + str(self.sigma)[:4] + \"__lr_\" + str(self.lr)[:6] + \".pt\")  \n",
        "            torch.save(self.to_dict(),path +\"/\" + self.id_ + \"/\"+\"state_gen_{:02d}.pt\".format(self.generation))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sgIPIxoDHiLZ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5pepsXaD8ei"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--num_candidates', type=int, default=2, metavar='', #4\n",
        "                        help='Number of candidates elements among which to choose one.')\n",
        "    parser.add_argument('--lr', type=float,  default=0.1,\n",
        "                        metavar='', help='ES learning rate.')\n",
        "    parser.add_argument('--lr_decay', type=float,  default=0.99, metavar='', #reduce if want to converge faster\n",
        "                        help='ES and learning rate decay.') \n",
        "    parser.add_argument('--sigma', type=float,  default=0.4, metavar='',\n",
        "                        help='ES sigma: modulates the amount of noise used to populate each new generation, the higher the more the entities will vary')\n",
        "    parser.add_argument('--sigma_decay', type=float,  default=0.95, metavar='', \n",
        "                        help='Simga rate decay.') \n",
        "    parser.add_argument('--generations', type=int, default=4, #20\n",
        "                        metavar='', help='Number of generations that the ES will run.')\n",
        "    parser.add_argument('--population_size', type=int, default=4, metavar='', #16 /32\n",
        "                        help='Size of population (needs to be pair and be a multiple of choice_batch or will be approximated).')\n",
        "    parser.add_argument('--new_run', type=str, default=None, metavar='', \n",
        "                        help='If continue from a previous run, specify the path to the folder containing the run. e.g. 178../state_gen_00.pt')\n",
        "    parser.add_argument('--clear_output', type=bool, default=True, metavar='', \n",
        "                        help='If want to clear output while running true else false')\n",
        "\n",
        "    #---Parameters to save the folder ----\n",
        "    parser.add_argument('--folder', type=str, default='weights',\n",
        "                        metavar='', help='folder to store the evolved weights')\n",
        "    \n",
        "    args = parser.parse_args(args)\n",
        "    print(args)\n",
        "\n",
        "    assert args.num_candidates <= args.population_size\n",
        "\n",
        "    if not exists(args.folder): \n",
        "        mkdir(args.folder) \n",
        "        \n",
        "\n",
        "    print('\\nLoading Embroidery Model')\n",
        "    # Initialise the EvolutionStrategy class\n",
        "    print('\\nInitialising Evolution Strategies')\n",
        "    es = EvolutionStrategyStatic( args=args)\n",
        "\n",
        "    # Start the evolution\n",
        "    print('\\nRunning ES for {} generations, solutions saved at {}'.format(args.generations, args.folder))\n",
        "    es.run(args.generations, path=args.folder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UYORh-23plkz"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VRBeVLBZpX_a",
        "outputId": "246f0516-7ded-4164-ee84-d57241c54f98"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # Replace the list below with the command-line arguments you want to use\n",
        "    args = [\"--num_candidates\",\"2\",\"--generations\",\"100\",\"--population_size\",\"4\",\"--lr_decay\", \"0.95\",\"--lr\",\"1\",\"--sigma\",\"0.6\",\"--sigma_decay\",\"0.96\",\"--clear_output\",\"False\"]\n",
        "    main(args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Show results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-- set up for show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "weight_nr = \"1682684774\" #edit this\n",
        "nr_gen = 18 #edit this\n",
        "folder_path = \"./weights/\"+weight_nr\n",
        "files = os.listdir(folder_path) # use os.listdir() to get a list of all the files in the folder\n",
        "files.sort()\n",
        "print(files)\n",
        "#print(torch.load(\"weights/\"+weight_nr+\"/\"+files[0]))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-- show weight development for a random ing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(nr_gen*2+1,nr_gen*3+1):\n",
        "    network = torch.load(\"weights/\"+weight_nr+\"/\"+files[i])\n",
        "    print(shift_params(network(\"random_prompts/A parent reading a bedtime story to their child_50.png\")))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-- show difference in the params for each gen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_path = \"./weights/\"+weight_nr\n",
        "files = os.listdir(folder_path) # use os.listdir() to get a list of all the files in the folder\n",
        "files.sort()\n",
        "\n",
        "data=[]\n",
        "for i in range(nr_gen*2+1,nr_gen*3+1):\n",
        "    temp = torch.load(\"weights/\"+weight_nr+\"/\"+files[i])\n",
        "    data.append(temp(\"random_prompts/A parent reading a bedtime story to their child_50.png\"))\n",
        "\n",
        "def helper(data,data2):\n",
        "    temp = []\n",
        "    shifted = shift_params(data)\n",
        "    shifted2 = shift_params(data2)\n",
        "    for i in range(4):\n",
        "        temp.append(round(shifted[i].item()-shifted2[i].item(),1))\n",
        "    return temp\n",
        "\n",
        "for i in range(len(data)):\n",
        "    if(i != 0):\n",
        "        temp = helper(data[i-1],data[i])\n",
        "        print(i,\":\",temp, \" -- \", max(map(abs,temp)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-- show images for each gen "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" import os\n",
        "weight_nr = \"1682684774\" #edit this\n",
        "nr_gen = 18 #edit this\n",
        "folder_path = \"./weights/\"+weight_nr\n",
        "files = os.listdir(folder_path) # use os.listdir() to get a list of all the files in the folder\n",
        "files.sort()\n",
        "folder_path = \"./weights/\"+weight_nr\n",
        "files = os.listdir(folder_path) # use os.listdir() to get a list of all the files in the folder\n",
        "files.sort()\n",
        "print(files)\n",
        "\n",
        "files = files[nr_gen*2+1:nr_gen*3+1]\n",
        "print(files)\n",
        "networks = []\n",
        "for i in range(nr_gen):\n",
        "    networks.append(torch.load(\"weights/\"+weight_nr+\"/\"+files[i])) \"\"\"\n",
        "nr_gen = 1\n",
        "def init_weights(): #get initial weights, which is set randomly\n",
        "    canny=CannyModel()\n",
        "    weight_dim=512*16+16*16+16*4\n",
        "    bias_dim=16+16+4\n",
        "    evolved_parameters=(2*np.random.rand((weight_dim+bias_dim))-1)\n",
        "    nn.utils.vector_to_parameters(torch.tensor(evolved_parameters, dtype=torch.float32),  canny.parameters())\n",
        "    return canny \n",
        "networks = []\n",
        "for i in range(5):\n",
        "    networks.append(init_weights())\n",
        "\n",
        "#prompts_ = [\"A parent reading a bedtime story to their child\",\"A flight attendant assisting passengers on a plane\",\"A scientist observing a chemical reaction in a lab\",'A businessman shaking hands with a client in an office', 'A businessperson closing a major deal in a boardroom', 'A businessperson giving a powerful and inspiring presentation', 'A child blowing out birthday candles on a cake']\n",
        "prompts_ = ['A businessman shaking hands with a client in an office_50.png', 'A businessperson closing a major deal in a boardroom_50.png', 'A businessperson giving a powerful and inspiring presentation_50.png', 'A child blowing out birthday candles on a cake_50.png', 'A couple dancing at a wedding reception_50.png', 'A dancer practicing in a dimly lit studio_50.png', 'A doctor comforting and treating a sick patient in a hospital_50.png', 'A group of friends playing volleyball on the beach_50.png', 'A group of people enjoying a concert in a stadium_50.png', 'A group of tourists taking in the sights and sounds of a new city_50.png', 'A man riding a bike through a city park_50.png', 'A mother and daughter baking cookies in the kitchen_50.png', 'A person practicing yoga on a beach at sunrise_50.png', 'A person skateboarding in a city park with skyscrapers in the background_50.png', 'A salesperson closing a deal with a customer_50.png', 'A scientist observing a chemical reaction in a lab_50.png', 'A soldier bravely serving their country in a warzone_50.png', 'A student studying and preparing for a big exam_50.png', 'A veterinarian examining a sick animal_50.png', 'A woman holding a bouquet of flowers and a puppy in a park_50.png']\n",
        "\n",
        "# Set the dimensions of each image and the number of rows and columns\n",
        "image_width = 512\n",
        "image_height = 512\n",
        "num_columns = len(prompts_)\n",
        "num_rows = len(networks)\n",
        "\n",
        "# Create a blank image with the desired size\n",
        "total_width = image_width * num_columns\n",
        "total_height = image_height * num_rows\n",
        "combined_image = Image.new('RGB', (total_width, total_height))\n",
        "human_seg = []\n",
        "other_seg = []\n",
        "# Paste the three images onto the blank image\n",
        "for i in range(num_rows):\n",
        "    for j in range(num_columns):\n",
        "        img_name = prompts_[j]\n",
        "        image_number = i * num_columns + j + 1\n",
        "        filename = \"random_prompts/\"+img_name\n",
        "        network_param_candidate = networks[i]\n",
        "        if (i == 0):\n",
        "            image = Image.open(filename)\n",
        "            other_img,human_img = segment(image)\n",
        "            other_seg.append(other_img)\n",
        "            human_seg.append(human_img)\n",
        "            print(filename)\n",
        "            canny_params = network_param_candidate.forward_1img(filename)\n",
        "            canny_params = shift_params(canny_params)\n",
        "            print(canny_params)\n",
        "            image=canny_edge(low_other=canny_params[2].item(),high_other=canny_params[3].item(),other_image=other_img,low_human=canny_params[0].item(),high_human=canny_params[1].item(),human_image=human_img)\n",
        "        else:\n",
        "            other_img = other_seg[j]\n",
        "            human_img = human_seg[j]\n",
        "            canny_params = network_param_candidate.forward_1img(filename)\n",
        "            canny_params = shift_params(canny_params)\n",
        "            print(canny_params)\n",
        "            image=canny_edge(low_other=canny_params[2].item(),high_other=canny_params[3].item(),other_image=other_img,low_human=canny_params[0].item(),high_human=canny_params[1].item(),human_image=human_img)\n",
        "       \n",
        "        image = image.resize((image_width, image_height))\n",
        "        combined_image.paste(image, (j * image_width, i * image_height))\n",
        "    print()\n",
        "    \n",
        "# Save the combined image\n",
        "combined_image.show()\n",
        "combined_image.save(weight_nr+ \".png\")\n",
        "plt.imshow(combined_image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting of sigma, lr, update_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nr_gen = 50\n",
        "lr = 0.6\n",
        "lr_decay = 0.955\n",
        "sigma = 0.6\n",
        "sigma_decay = 0.96\n",
        "update_factor = 0\n",
        "pop_size = 32\n",
        "\n",
        "sigma_list = []\n",
        "lr_list = []\n",
        "update_factor_list = []\n",
        "\n",
        "for i in range(nr_gen):\n",
        "    update_factor = lr / (pop_size * sigma)\n",
        "    sigma *= sigma_decay\n",
        "    lr *= lr_decay\n",
        "    sigma_list.append(sigma)\n",
        "    lr_list.append(lr)\n",
        "    update_factor_list.append(update_factor)\n",
        "\n",
        "\n",
        "gen_arr = np.arange(0,nr_gen,1)\n",
        "plt.title(\"Line graph\")\n",
        "plt.plot(gen_arr, sigma_list, color=\"red\")\n",
        "for i in range(len(sigma_list)):\n",
        "    plt.annotate(f'{sigma_list[i]:.1f}', (gen_arr[i], sigma_list[i]))\n",
        "    \n",
        "# Add title and labels\n",
        "plt.title('Sigma plt')\n",
        "plt.xlabel('Generations')\n",
        "plt.ylabel('Sigma')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "    \n",
        "\"\"\" plt.show()\n",
        " \"\"\"\"\"\" \n",
        "gen_arr = np.arange(0,nr_gen,1)\n",
        "plt.title(\"Line graph\")\n",
        "plt.plot(gen_arr, lr_list, color=\"red\")\n",
        "for i in range(len(lr_list)):\n",
        "    plt.annotate(f'{lr_list[i]:.1f}', (gen_arr[i], lr_list[i]))\n",
        "# Add title and labels\n",
        "plt.title('LR plt')\n",
        "plt.xlabel('Generations')\n",
        "plt.ylabel('LR')\n",
        "    \n",
        "plt.show() \"\"\"\n",
        "\n",
        "gen_arr = np.arange(0,nr_gen,1)\n",
        "plt.title(\"Line graph\")\n",
        "plt.plot(gen_arr, update_factor_list, color=\"red\")\n",
        "for i in range(len(update_factor_list)):\n",
        "    plt.annotate(f'{update_factor_list[i]:.3f}', (gen_arr[i], update_factor_list[i]))\n",
        "# Add title and labels\n",
        "plt.title('Update factor plt')\n",
        "plt.xlabel('Generations')\n",
        "plt.ylabel('Update factor')\n",
        "    \n",
        "plt.show()    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lsrzMdh_lEym",
        "KnRsSlZOHI8x",
        "XvXfl5nkHEEI",
        "7oFcJVbLlz5Q",
        "R6UrP_TxHajd",
        "sgIPIxoDHiLZ",
        "8jg3eBFqrfSk"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
